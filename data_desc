Importing data with pandas:
CSV format -> comma separated values
.read_CSV() method - can read in files with columns separated by commas into a pandas data frame
Reading data in pandas can be done quickly in three lines:
1. import pandas
2. define a variable with a file path
3. usa the .read_CSV() method to import data

The .read_csv() method assumes the data contains a header.

After a reading a dataset, it's good idea to look at the data frame. We could print it, or just print the headers.
df.head(n) - to show the first n rows of data frame
df.tail(n) - to show the bottom n rows of data frame

Pandas automatically set the column header as a list of integers, if the headers were set to None
In order to replace the column names with something more meaningful, we need to have a list wih the headers,
and then assign df columns to this list.
headers = []
df.columns = headers

Exporting data with pandas:
We can preserve progresses anytime by saving modified dataset using:
path="defined_path_with_file_name"
df.to_scv(path, index=False) -> index=False means, that row names will not be written

Pandas also supports other formats than csv:
csv     pd.read_csv()   df.to_csv()
json   pd.read_json()   df.to_json()
Excel  pd.read_excel() df.to_excel()
sql     pd.read_sql()   df.to_sql()
hdf     pd.read_hdf()   df.to_hdf()
...

Data types:
- Pandas Type -                 Native Python Type -                            Description
- object -                      string -                                        numbers and strings
- int64 -                       int -                                           numeric characters
- float64 -                     float -                                         numeric characters with decimals
- datetime64, timedelta[ns] - /datetime module in Python's standard library/ - time data

Why check data types?
- potential info and type mismatch
- compatibility with python methods

df.dtypes() - is used to check data types

df.describe() - returns a statistical summary
    - count- number of terms
    - mean - average value
    - std - standard deviation
    - min - min value
    - 25% - boundary of each quartiles
    - 50% -
    - 75% -
    - max - max value
By default the method skips rows and columns, that don't contain numbers. But it's possible to make the .describe() method
worked for object type as well. Then we need do add an argument to the describe() method:
df.describe(include=all)
We also get additional set of data:
    - unique - number of distinct objects in column
    - top - most frequently occurring object
    - freq - number of times the top object appears in the column
    NaN - not a number

df.replace(v1, v1) - replaces value 1 with value 2 in dataframe
df.dropna() - removes the missing values
df.info() - provides a concise summary of the DataFrame - column names and data types
df.columns - a list of headers

We can select the columns of a dataframe by indicating the name of each column. For example, you can select the three columns as follows:
dataframe[[' column 1 ',column 2', 'column 3']]
Where "column" is the name of the column, we can apply the method ".describe()" to get the statistics of those columns as follows:
dataframe[[' column 1 ',column 2', 'column 3'] ].describe()

Accessing Databases with Python:
User <--> Python Programs <-API calls-> DBMS
The Python code connects to the database, using API calls.

1. Application program --CONNECT(db, user, pswd)--> DBMS
2. Application program --SEND("update....")-------> DBMS
3. Application program --EXECUTE()----------------> DBMS
4. Application program --STATUS_CHECK()-----------> DBMS
5. Application program <-OK------------------------ DBMS
6. Application program --DISCONNET()--------------> DBMS

An application programming interface is a set of functions that we can call to get access to some type of service.
The SQL API consists of library function calls as an application programming interface, API, for the DBMS.
To pass SQL statements to the DBMS, an application program calls functions in the API, and it calls other functions to
retrieve query results and status information from the DBMS:
1. The application program begins its database access with one or more API calls that connect the program to the DBMS.
2. To send the SQL statement to the DBMS, the program builds the statement as a text string in a buffer
3. and then makes an API call to pass the buffer contents to the DBMS.
4./5. The application program makes API calls to check the status of its DBMS request and to handle errors.
6. The application program ends its database access with an API call that disconnects it from the database

What is a DB-API?
DB-API is Python's standard API for accessing relational databases.
It's a standard, that allows us to write a single program that works with multiple kinds of relational databases
instead of writing a separate program for each one.

Two main concepts of the Python DB-APi:
1. Connection objects
    - Database connections
    - Manage transactions
2. Query objects
    - Cursor objects are used to run queries and fetch results
        We open a cursor object and then run queries. Cursors are used to scan through the results of a database.

What are Connection method? Method used with Connection objects:
.cursor() - returns a new cursor object using the connection
.commit() - commits any pending transaction to the database
.rollback() - causes the database to roll back to the start of any pending transaction
.close() - closes a database connection

Data Pre-processing (also known as data cleaning or data wrangling)
It's a process of converting or mapping data from initial "raw" form into another format, in order to prepare the data
for further analysis.

In Python, we usually perform operations along columns.
Each row of the column represents a sample.
We access a column by specifying the name of the column, i.e.:
df["symboling"]
df["body-style"]

We can add a value to each entry of a column, i.e to add one to each "symboling" entry:
df["symboling"] = df["symboling"] + 1

Dealing with missing values in Python
Missing value - when no data value is stored for a variable (feature) in an observation.
Could be represented as:
- ?
- N/A
- 0
- just blank cell

How to deal with missing data?
- check with the data collection sources
- drop the missing values:
    - drop the variable
    - drop the data entry
- replace the missing values (with a guess of what the data should be)
    - replace it with an average (of similar datapoints)
    - replace it by frequency
    - replace it based on other functions
- leave it as missing data


How to drop missing values in Python:
dataframe.dropna() - is used to drop rows or columns that contain missing values like NaN
    we need do specify axis:
    axis=0 -> drops the entire row
    axis=1 -> drops the entire column   that contains the missing values

i.e.: in order to drop the cars, for which there is no data in "price" column:

df.dropna(subset=["price"], axis=0, inplace=True)
Setting the argument inplace to True, allows the modification to be done on the data set directly -> writes the result back into the dataframe

df.dropna(subset=["price"], axis=0)
This line of code doesn't change the data frame, but it is a good way to make sure that we perform the correct operation.

Documentation:
http://pandas.pydata.org/


How to replace missing values in Python:
dataframe.replace()
dataframe.replace(missing_value, new_value)

Assuming, that we want to replace the missing value of the variable "normalized-losses" by the mean value of the variable:
1. Calculate the mean of the column:
    mean = df["normalises-losses"].mean()
2. Use the method .replace()
    df["normalises-losses"].replace(/np. nan/, mean)



Data formatting in Python:
Data formatting means bringing data into a common standard of expression, in order to allow users to make meaningful comparison.

Applying calculations to an entire column:
- for example: convert units from "mpg" (miles per gallon) to "l/100km" in the column "city-mpg"
    df["city-mpg"] = 237 / df["city-mpg"]
    and rename the column:
    df.rename(columns=("city-mpg": "city-l/100km"), inplace=True)

Incorrect data types:
dataframe.dtypes() - to identify data type:
dataframe.astype() - to convert data type:
    for example: convert data type to integer in column "price"
    df["price"] = df["price"].astype("int")

Data Normalisation in Python:
data normalization: uniform the features value with different range.
By making the ranges consistent between variables, normalization enables a fair comparison between the different features, making sure they have the same impact.
Normalized data:
- similar range value
- similar intrinsic influence on analytical model

There are several ways to normalize data:
1. Simple feature scaling:
    Xnew = Xold/Xmax:                   this makes the new values range between 0 and 1
2. Min-Max:
    Xnew = (Xold-Xmin)/(Xmax-X-min)     this makes the new values range between 0 and 1
3. Z-score (or standard score):
    Xnew = (Xold - μ)/σ                 typically range between -3 and 3 (but can be higher or lower)
    μ - average of the feature
    σ - standard deviation

Normalization of the "length" value:
    Simple feature scaling in Python:
df["length"] = df["lenght"] / df["length"].max()

    Min-max in Python:
df["length"] = (df["length"] - df["length"].min()) /
                (df["length"].max() - df["length"].min())

    Z-score in Python:
df["length"] = (df["length"] - df["length"].mean()) /
                df["length"].std()

.min() - returns minimal value of the feature in the dataset
.max() - returns maximal value
.mean() - returns average value
.std() - returns standard deviation